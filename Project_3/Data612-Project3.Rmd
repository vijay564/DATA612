---
title: "Data612-Project3"
author: "Vijaya Cherukuri, Habib Khan"
date: "6/21/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Objective

• Your task is implement a matrix factorization method—such as singular value decomposition (SVD) or Alternating Least Squares (ALS)—in the context of a recommender system.

• You may approach this assignment in a number of ways. You are welcome to start with an existing recommender system written by yourself or someone else. Remember as always to cite your sources, so that you can be graded on what you added, not what you found.


# Introduction

SVD can be thought of as a pre-processing step for feature engineering. You might easily start with thousands or millions of items, and use SVD to create a much smaller set of “k” items (e.g. 20 or 70).

• This project is based on the work done in [Project 2](https://rpubs.com/habibkhan89/628577)

• In this project we will add SVD to further explore the recommender system.
I have used the `recommenderlab` package.


The goal of this assignment is to implement a matrix factorization method such as singular value decomposition (SVD) in the context of a recommender system. We will work with an existing recommender system build by us for [previous assignment](https://github.com/habibkhan89/Data-612/blob/master/Data%20612%20-%20Project%202.rmd).  

### Loading Libraries

```{r library, warning=FALSE, message=FALSE}

library(tidyverse)
library(kableExtra)
library(knitr)
library(recommenderlab)
```

### Overview of dataset

Although there are lots of dataset for recommender systems but for sake of simplicity I will include MovieLens dataset. This data was collected through the MovieLens web site (movielens.umn.edu) during the seven-month period from September 19th, 1997 through April 22nd, 1998. The data set contains about 100,000 ratings (1-5) from 943 users on 1664 movies. Movie metadata is also provided in MovieLenseMeta. It is a built-in dataset in "recommenderlab" library.

```{r data,}
# ratings of first five users and five movies
data("MovieLense")
data.frame(as(MovieLense, "matrix")[1:5, 1:5]) %>% 
  kable(col.names = colnames(MovieLense)[1:5]) %>% 
  kable_styling(full_width = T)
```

# Data Exploration 
In this section, We will explore the dataset by visualizing it through graphs. By looking at the distribution of the ratings we can observer the ratings of 4 has the highest count. We plot the heat map of the rating matrix for easy identification.

```{r exploration}
# distribution of ratings
MovieLense@data %>% 
  as.vector() %>% 
  as_tibble() %>% 
  filter_all(any_vars(. != 0)) %>% 
  ggplot(aes(value)) + 
  geom_bar() +
  labs(title = "Distribution of the ratings", y = "", x = "Ratings") +
  theme_minimal()
# heatmap of the rating matrix
image(MovieLense@data, main = "Heat map of the rating matrix")
```

# Data Preparation
In this section we will prepare data for most accurate model. First, we will normalize the data because having users who give high (or low) ratings to all their movies might bias the results. We can remove this effect by normalizing the data in such a way that the average rating of each user is 0. Next, We impute the missing values of each column using the mean of the respective column. Finally, We select the most relevant data about movies that have been viewed only a number of times because otherwise their ratings might be biased because of lack of data. Also users who rated only a few movies, their ratings might be biased as well.   
```{r preparation,}
# normalize the data
ratings_movies <- normalize(MovieLense)
rcm <- colMeans(ratings_movies)
ratings_movies <- as(ratings_movies, "matrix")
# imputing missing values
ratings_movies[is.na(ratings_movies)] <- rcm
ratings_movies <-  as(ratings_movies, "realRatingMatrix")
# selecting the most relevant data
ratings_movies <- ratings_movies[rowCounts(ratings_movies) > 50, 
                                 colCounts(ratings_movies) > 100]
```

## Feature Engineering
A matrix factorization involves describing a given matrix using its constituent elements. Perhaps the most known and widely used matrix decomposition method is the Singular-Value Decomposition, or SVD. A popular application of SVD is for dimensionality reduction. Data with a large number of features, such as more features (columns) than observations (rows) may be reduced to a smaller subset of features that are most relevant to the prediction problem.
```{r fe1, }
# compute SVD of the rating matrix 
ratings_svd <- as(ratings_movies, "matrix")
ratings_svd <- svd(ratings_svd)
```

The singular value decomposition of a rating matrix is the factorization of the matrix into the product of three matrices $A = U\Sigma V^T$, where:   
  - A is an $m × n$ matrix (rating matrix)  
  - U is an $m × n$ orthogonal matrix (users-concepts similarity matrix)  
  - $\Sigma$  is an $n × n$ diagonal matrix (items-concepts similarity matrix)  
  - V is an $n × n$ orthogonal matrix (strength of each concepts)  
```{r fe2, }
# get U, V, Sigma
u <- ratings_svd$u
v <- ratings_svd$v
sigma <- ratings_svd$d
```

We can think of `U` as columns of concepts or genres, and rows as users. Each cell tells us how strongly or weakly the user corresponds to that concept. 
```{r fe3, }
# inspect U
u[1:5, 1:5] %>% 
  kable(col.names = c("Concept 1", "Concept 2", "Concept 3", "Concept 4", "Concept 5")) %>%
  kable_styling(full_width = F)
```

We can think of `V` as columns of concepts or genres, and rows as items, Each cell tells us how strongly or weakly the item corresponds to that concept. 
```{r fe4, }
# inspect v
v[1:5, 1:5] %>% 
  kable(col.names = c("Concept 1", "Concept 2", "Concept 3", "Concept 4", "Concept 5")) %>%
  kable_styling(full_width = F)
```

A limitation of SVD is that features may not map neatly to items. I think we can improve the mapping by sorting the items by group (genre) as seen in the [video](https://www.youtube.com/watch?v=P5mlg91as1c&list=PLLssT5z_DsK9JDLcT8T62VtzwyW9LNepV&index=47).

We can think of $\Sigma$ as strength of concepts. The value is in descending order and every number tells us how strong the concepts are. Based on this value, we can reduce the dimension of our dataset. 
```{r fe5, }
# inspect sigma
head(sigma, 5) %>% kable(col.names = "Strength of Sigma") %>% kable_styling(full_width = F)
# visualize the sigma
plot(sigma)
```

The best way to reduce the dimensionality of the three matrices is to set the smallest of the singular values to zero. If we set the $s$ smallest singular values to 0, then we can also eliminate the corresponding $s$ columns of U and V. A useful rule of thumb is to retain enough singular values to make up 90% of the energy in $\Sigma$. That is, the sum of the squares of the retained singular values should be at least 90% of the sum of the squares of all the singular values. As we can see below, setting 300 smallest singular values to 0 still retains 94.7% of the energy in $\Sigma$.
```{r}
# reduce dimension
a <- sum(sigma ^ 2)
b <- sum((sigma[1:(length(sigma) - 300)])^2)
print(paste0("The energy in sigma after dimension reduction is ", round((b/a)*100, 2), "%"))
```

Finally, we use the RMSE equation to calculate the difference between the new matrix and the original matrix. 
```{r}
# reduced matrix
u1 <- as.matrix(u[, 1:643])
d1 <- as.matrix(diag(sigma)[1:643, 1:643])
v1 <- as.matrix(v[, 1:643])
ratings_reduced <- u1 %*% d1 %*% t(v1)
# rmse of the matrices
sqrt(mean((ratings_reduced - as(ratings_movies, "matrix"))^2, na.rm = T))
```

## Reference

1) [Introduction to Recommender System](https://hackernoon.com/introduction-to-recommender-system-part-1-collaborative-filtering-singular-value-decomposition-44c9659c5e75)
2) [Youtube video](https://www.youtube.com/watch?v=P5mlg91as1c&list=PLLssT5z_DsK9JDLcT8T62VtzwyW9LNepV&index=47)
