---
title: "Data612-Project4"
author: "Vijaya Cherukuri, Habib Khan"
date: "6/29/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Objective

The goal of this assignment is give you practice working with accuracy and other recommender system metrics.

Deliverables

1. As in your previous assignments, compare the accuracy of at least two recommender system algorithms against your offline data.

2. Implement support for at least one business or user experience goal such as increased serendipity, novelty, or diversity.

3. Compare and report on any change in accuracy before and after you’ve made the change in #2.

4. As part of your textual conclusion, discuss one or more additional experiments that could be performed and/or metrics that could be evaluated only if online evaluation was possible. Also, briefly propose how you would design a reasonable online evaluation environment.

# Load required libraries

```{r echo=TRUE}
library(recommenderlab)  # Matrix/recommender functions
library(dplyr)           # Data manipulation
library(tidyr)           # Data manipulation
library(ggplot2)         # Plotting
library(tictoc)          # Operation timing
```


# Dataset Information

• The dataset is a product ratings for beauty products sold on Amazon.com. 
The dataset was downloaded from [Kaggle.com](https://www.kaggle.com/skillsmuggler/amazon-ratings).

• Original set contains 2,023,070 observations and 4 variables - User ID, Product ID, Rating (from 1 to 5), and Time Stamp. It covers 1,210,271 users and 249,274 products. In order to make the set more manageable it has been reduced to a smaller subset.

• The final ratings dataset used consists of 3562 x 12057 rating matrix of class ‘realRatingMatrix’ with 68565 ratings.

# Subsetting Dataset
Now let's see the process of reducing the data from the main dataset

Step 1) Import original file and select sample for project



```{r echo=TRUE}
ratings <- read.csv("E:/github/MS/DATA612/Project_4/ratings_Beauty.csv")
```


Step-2) Explore

```{r echo=TRUE}
head(ratings)
class(ratings$UserId); class(ratings$ProductId); class(ratings$Rating); class(ratings$Timestamp)
hist(ratings$Rating, col = "Blue")
```


Step-3) Convert to realRatingMatrix


```{r echo=TRUE}
ratingsMatrix <- sparseMatrix(as.integer(ratings$UserId), as.integer(ratings$ProductId), x = ratings$Rating)
colnames(ratingsMatrix) <- levels(ratings$ProductId)
rownames(ratingsMatrix) <- levels(ratings$UserId)
amazon <- as(ratingsMatrix, "realRatingMatrix")
```


Step-4) Explore

```{r echo=TRUE}
amazon 
hist(rowCounts(amazon), col = "Green")
table(rowCounts(amazon))
hist(colCounts(amazon), col = "Yellow")
table(colCounts(amazon))
```


Step-5) Select Subset 1 and Subset 2

```{r echo=TRUE}
( amazonShort <- amazon[rowCounts(amazon) > 10, colCounts(amazon) > 30] )
amazonShort <- amazon[ , colCounts(amazon) > 30]
amazonShort <- amazonShort[rowCounts(amazonShort) > 10, ]
amazonShort
```


Step-6) Check and Remove Empty Lines

```{r echo=TRUE}
table(rowCounts(amazonShort))
table(colCounts(amazonShort))
( amazonShort <- amazonShort[ , colCounts(amazonShort) != 0] )
```


Step-7) Convert to data frame and save as CSV file


```{r echo=TRUE}
df <- as.data.frame(as.matrix(amazonShort@data))
df$UserId <- rownames(df)
df <- df %>% gather(key = ProductId, value = Rating, -UserId) %>% filter(Rating != 0)
write.csv(df, "E:/github/MS/DATA612/Project_4/ratings_final.csv", row.names = FALSE)
```


# Data import and Data Preparation

Import the ratings_final dataset:

```{r echo=TRUE}
ratings <- read.csv("https://raw.githubusercontent.com/vijay564/DATA612/master/Project_4/dataset/ratings_final.csv")
ratingsMatrix <- sparseMatrix(as.integer(ratings$UserId), as.integer(ratings$ProductId), x = ratings$Rating)
colnames(ratingsMatrix) <- levels(ratings$ProductId)
rownames(ratingsMatrix) <- levels(ratings$UserId)
amazon <- as(ratingsMatrix, "realRatingMatrix")
```


# Train and Test sets

Split the dataset into test and train sets to build the model.

```{r echo=TRUE}
# Train/test split
set.seed(88)
eval <- evaluationScheme(amazon, method = "split", train = 0.8, given = 5, goodRating = 3)
train <- getData(eval, "train")
known <- getData(eval, "known")
unknown <- getData(eval, "unknown")
# Set up data frame for timing
timing <- data.frame(Model=factor(), Training=double(), Predicting=double())
```


# Recommender Models

Now, Let's build three different models

<strong> USER BASED COLLABORATIVE FILTERING </strong>

```{r echo=TRUE}
model_method <- "UBCF"
# Training
tic()
modelUBCF <- Recommender(train, method = model_method)
t <- toc(quiet = TRUE)
train_time <- round(t$toc - t$tic, 2)
# Predicting
tic()
predUBCF <- predict(modelUBCF, newdata = known, type = "ratings")
t <- toc(quiet = TRUE)
predict_time <- round(t$toc - t$tic, 2)
timing <- rbind(timing, data.frame(Model = as.factor(model_method), 
                                   Training = as.double(train_time), 
                                   Predicting = as.double(predict_time))) 
# Accuracy
accUBCF <- calcPredictionAccuracy(predUBCF, unknown)
#resultsUBCF <- evaluate(x = eval, method = model_method, n = c(1, 5, 10, 30, 60))
```


<strong> RANDOM </strong>

```{r echo=TRUE}
model_method <- "RANDOM"
# Training
tic()
modelRandom <- Recommender(train, method = model_method)
t <- toc(quiet = TRUE)
train_time <- round(t$toc - t$tic, 2)
# Predicting
tic()
predRandom <- predict(modelRandom, newdata = known, type = "ratings")
t <- toc(quiet = TRUE)
predict_time <- round(t$toc - t$tic, 2)
timing <- rbind(timing, data.frame(Model = as.factor(model_method), 
                                   Training = as.double(train_time), 
                                   Predicting = as.double(predict_time))) 
# Accuracy
accRandom <- calcPredictionAccuracy(predRandom, unknown)
#resultsRandom <- evaluate(x = eval, method = model_method, n = c(1, 5, 10, 30, 60))
```


<strong> SVD </strong>

```{r echo=TRUE}
model_method <- "SVD"
# Training
tic()
modelSVD <- Recommender(train, method = model_method, parameter = list(k = 50))
t <- toc(quiet = TRUE)
train_time <- round(t$toc - t$tic, 2)
# Predicting
tic()
predSVD <- predict(modelSVD, newdata = known, type = "ratings")
t <- toc(quiet = TRUE)
predict_time <- round(t$toc - t$tic, 2)
timing <- rbind(timing, data.frame(Model = as.factor(model_method), 
                                   Training = as.double(train_time), 
                                   Predicting = as.double(predict_time))) 
# Accuracy
accSVD <- calcPredictionAccuracy(predSVD, unknown)
#resultsSVD <- evaluate(x = eval, method = model_method, n = c(1, 5, 10, 30, 60))
```


# Compairing Models

As we have build all three models for the dataset, now we can proceed with compairing the accuracy for all three models

```{r echo=TRUE}
accuracy <- rbind(accUBCF, accRandom)
accuracy <- rbind(accuracy, accSVD)
rownames(accuracy) <- c("UBCF", "Random", "SVD")
knitr::kable(accuracy, format = "html") %>%
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover"))
```

As we review the accuracy scores above for UBCF, Random, SVD models, we see that Random has the lowest accuracy than UBCF and SVD. Whereas, UBCF and SVD models accuracy figures are quite close to each other. It is not surprising that random recommendations are not as accurate as recommendations based on prior ratings.



# ROC curve

Now we can we can review ROC curve and Precision-Recall plot for all three models.

```{r echo=TRUE}
models <- list(
  "UBCF" = list(name = "UBCF", param = NULL),
  "Random" = list(name = "RANDOM", param = NULL),
  "SVD" = list(name = "SVD", param = list(k = 50))
  )
evalResults <- evaluate(x = eval, method = models, n = c(1, 5, 10, 30, 60))
```


```{r echo=TRUE}
# ROC Curve
plot(evalResults, 
     annotate = TRUE, legend = "topleft", main = "ROC Curve")
```


```{r echo=TRUE}
# Precision-Recall Plot
plot(evalResults, "prec/rec", 
     annotate = TRUE, legend = "topright", main = "Precision-Recall")
```

UBCF performs better than SVD and considerably better than the Random model.

• Now, Let us see the training and prediction time.

• From the table below we can see that the UBCF model can be created fairly quickly, but predicting results takes considerable time. The Random model is pretty efficient all around. The SVD model takes longer to build than to predict, but altogether it is quicker than the UBCF model. 

```{r echo=TRUE}
rownames(timing) <- timing$Model
knitr::kable(timing[, 2:3], format = "html") %>%
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover"))
```


# Implement Support for Business/User Experiance Goal

• Since UBCF and SVD models's accuracy scores were similar and they also performed better compared to Random model, let's create a hybrid model consisting of UBCF and SVD models.

• It may not always be desirable to recommend products that are likely to be most highly rated by a user. Recommending somewhat unexpected products may improve user experience, expand user preferences, provide additional knowledge about a user. 

• In order to make sure that most of recommendations are still likely to be highly rated we only allow very minor influence of the Random model (0.99 vs. 0.01 weight between UBCF and Random models).


```{r echo=TRUE}
model_Hybrid <- HybridRecommender(
    modelUBCF, 
    modelRandom, 
    weights = c(0.99, 0.01))
pred_Hybrid <- predict(model_Hybrid, newdata = known, type = "ratings")
( accHybrid <- calcPredictionAccuracy(pred_Hybrid, unknown) )
```

<strong> Comprison of the accuracy </strong>

The accuracy has gone down. It is not as bad as with purely random model, but clearly not as good as UBCF or SVD models. However, the goal here is to influence user experience rather than make the most accurate model, so we need to employ different metrics.

Let us look at top 10 recommendations for the first user in the test set.


```{r echo=TRUE}
pUBCF <- predict(modelUBCF, newdata = known[1], type = "topNList")
pHybrid <- predict(model_Hybrid, newdata = known[1], type = "topNList")
```

```{r echo=TRUE}
pUBCF@items
```


```{r echo=TRUE}
pHybrid@items
```

Now as we see, the Hybrid model includes most of the items recommended by the UBCF model, but there are new items and the order is different.


# Conclusion

• In this project we have build three different recommender system algorithms and compared the accuracy of all the three different models. Similar process can be employed to compare additional models or to adjust model parameters to find the most optimal model. 

<strong>Additional experiments that could be performed</strong>


• One of the approaches in measuring success of diversification may be<strong> A/B testing.</strong> Users are randomly divided into two groups and each group is offered a slightly different experience. For instance, one group may get recommendations only from the UBCF model while the other group will get recommendations from the hybrid model. 

• User experience is measured in some way. The least instrusive way is to monitor user interaction. In this example of Amazon products, a click on a recommendation suggested by the random element of the model will point to the fact that the hybrid model provides valuable recommendations. 

• Of course, it is possible to track other metrics - products bought, time spent on product page, amount spent, etc. The basic idea is to see if the hybrid model provides meaningful improvement to the basic model.

• It is important to have objective measures when building and optimizing data science models. Evaluation of a model that returns highly relevant, but redundant recommendations should reflect that the model may score poorly in user experience. 

• One of the approaches to measure diversity is described in [Novelty and Diversity in Information Retrieval Evaluation](https://plg.uwaterloo.ca/~gvcormac/novelty.pdf). This or similar measurement should be incorporated in projects of this type.